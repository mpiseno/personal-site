<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Michael  Piseno | Gradient of Softmax Cross-Entropy Loss</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.png">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/blog/2021/softmax-gradient/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

  <script src="/assets/js/theme.js"></script>
  <!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Michael</span>   Piseno
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/experience/">
                experience
                
              </a>
          </li>
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                teaching
                
              </a>
          </li>
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Gradient of Softmax Cross-Entropy Loss</h1>
    <p class="post-meta">February 4, 2021</p>
  </header>

  <article class="post-content">
    <p>There’s been a considerable number of students who have trouble deriving the gradient of the softmax cross-entropy loss function and translating it into code. In this post, I’ll derive it step-by-step so that you all have an easier time translating it to code, but I will not provide code because that’s part of the homework.</p>

<h2 id="notation">Notation</h2>

<p>We will denote the input to the softmax function (i.e. the logits) \(\mathbf{z}\) and the softmax function \(S(\mathbf{z})\) which produces a vector output \(\mathbf{s} = S(\mathbf{z})\). Let there be \(k\) classes so that \(\mathbf{z, s} \in \mathbb{R}^{k}\) and denote the index of the correct class \(c\). Recall that the softmax function produces a probability distribution over the classes. We denote the true distribution of the classes as \(\mathbf{y}\), which is a one-hot vector where index \(c\) is set to 1 (i.e. \(\mathbf{y}_{c} = 1\) and \(\mathbf{y}_{i}\) = 0 for \(i \neq c\)).</p>

<p>Let \(L(\mathbf{s})\) be the cross-entropy loss function defined as</p>

\[L(\mathbf{s}, \mathbf{y}) = -\sum_{i}\mathbf{y}_{i} \log \mathbf{s}_{i}\]

<h2 id="computing-the-gradient">Computing the Gradient</h2>

<p>First let’s simplify the cross-entropy loss function by using the fact that \(\mathbf{y}\) is a one-hot vector.</p>

\[\begin{align*}
    L(\mathbf{s}, \mathbf{y}) &amp;= -\sum_{i}\mathbf{y}_{i} \log \mathbf{s}_{i} \\
    &amp;= -\log \mathbf{s}_{c}
\end{align*}\]

<p>Now we can use the chain rule to note that \(\frac{\partial L}{\partial \mathbf{z}} = \frac{\partial L}{\partial \mathbf{s}_{c}}\frac{\partial \mathbf{s}_{c}}{\partial \mathbf{z}}\). \(\frac{\partial L}{\partial \mathbf{s}_{c}}\) is just \(\frac{-1}{\mathbf{s}_{c}}\). \(\frac{\partial \mathbf{s}_{c}}{\partial \mathbf{z}}\) is slightly more complicated. We will compute it by considering the \(i\)th entry - that is \(\frac{\partial \mathbf{s}_{c}}{\partial \mathbf{z}_{i}}\).</p>

<p>If \(i = c\) then we have</p>

\[\begin{align*}
    \frac{\partial \mathbf{s}_{c}}{\partial \mathbf{z}_{i}} &amp;= \frac{\partial}{\partial \mathbf{z}_{i}} \frac{e^{\mathbf{z}_{c}}}{\sum_{j}e^{\mathbf{z}_{j}}} \\
    &amp;= \frac{e^{\mathbf{z}_{c}}\sum_{j}e^{\mathbf{z}_{j}} - e^{\mathbf{z}_{c}}e^{\mathbf{z}_{c}}}{(\sum_{j}e^{\mathbf{z}_{j}})^{2}} \\
    &amp;= \frac{e^{\mathbf{z}_{c}}}{\sum_{j}e^{\mathbf{z}_{j}}}\frac{\sum_{j}e^{\mathbf{z}_{j}} - e^{\mathbf{z}_{c}}}{\sum_{j}e^{\mathbf{z}_{j}}} \\
    &amp;= \mathbf{s}_{c}(1 - \mathbf{s}_{c})
\end{align*}\]

<p>If \(i \neq c\), we get</p>

\[\begin{align*}
    \frac{\partial \mathbf{s}_{c}}{\partial \mathbf{z}_{i}} &amp;= \frac{\partial}{\partial \mathbf{z}_{i}} \frac{e^{\mathbf{z}_{c}}}{\sum_{j}e^{\mathbf{z}_{j}}} \\
    &amp;= \frac{-e^{\mathbf{z}_{i}}e^{\mathbf{z}_{c}}}{(\sum_{j}e^{\mathbf{z}_{j}})^{2}} \\
    &amp;= -\mathbf{s}_{i}\mathbf{s}_{c}
\end{align*}\]

<p>Now we can return to computing the full gradient \(\frac{\partial L}{\partial \mathbf{z}}\).</p>

\[\begin{align*}
    \frac{\partial L}{\partial \mathbf{z}} &amp;= \frac{\partial L}{\partial \mathbf{s}_{c}}\frac{\partial \mathbf{s}_{c}}{\partial \mathbf{z}} \\
    &amp;= \frac{-1}{\mathbf{s}_{c}}
    \begin{bmatrix} -\mathbf{s}_{1}\mathbf{s}_{c} &amp; -\mathbf{s}_{2}\mathbf{s}_{c} &amp; ... &amp; \mathbf{s}_{c}(1 - \mathbf{s}_{c}) &amp; ... &amp; -\mathbf{s}_{k}\mathbf{s}_{c} \end{bmatrix} \\
    &amp;= \begin{bmatrix} \mathbf{s}_{1} &amp; \mathbf{s}_{2} &amp; ... &amp; (\mathbf{s}_{c} - 1) &amp; ... &amp; \mathbf{s}_{k} \end{bmatrix}
\end{align*}\]

<p>So the gradient of the softmax cross-entropy amounts to subtracting 1 from value at the index of the correct class! We also could have computed the full row vector \(\frac{\partial L}{\partial \mathbf{s}}\) and Jacobian matrix \(\frac{\partial \mathbf{s}}{\partial \mathbf{z}}\) and matrix multiplied them. However, the former is sparse, so it simplified the computation a lot to do it how we did above. Furthermore, the method above shows that when computing gradients for backpropagation, we can use the fact that the flow of computation is such that the entries of \(\mathbf{s}\) that are not at the correct class’s index do not contribute to the loss, so we don’t need to consider them.</p>

<p>Hopefully this helped you guys out. Stay positive and test negative y’all.</p>


  </article>

  

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2021 Michael  Piseno.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
